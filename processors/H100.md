---
aliases:
  - H200
title: NVIDIA H100
tags:
  - GPU
---
H100 is NVIDIA's first Hopper-based GPU.

## Specifications

Each GPU has:

- 66 TPCs
	- 132 SMs (2 per [[GPU terminology decoder ring|TPC]])
	- 16,896 FP32 cores (128 per [[GPU terminology decoder ring|SM]])
	- 528 4th generation [[tensor cores]] (4 per [[GPU terminology decoder ring|SM]])
	- 1.83 GHz (low precision), 1.98 GHz (high precision)
	- 2:4 [[structured sparsity]]
- 96 GB HBM3 (? stacks)
	- 3.35 TB/s (max)
- 900 GB/s NVLink (D2D)
- 128 GB/s PCIe Gen5 (H2D)
- 700 W maximum

H200 GPUs are the same as H100 GPUs, just with 144 GB of HBM3e (more and faster HBM).

## Performance

The following are theoretical maximum performance in TFLOPS:[^1]

| Data Type | VFMA  | Matrix | Sparse |
| --------- | ----- | ------ | ------ |
| FP64      | 33.5  | 66.9   |        |
| FP32      | 66.9  |        |        |
| TF32      |       | 494.7  | 989.4  |
| FP16      | 133.8 | 989.4  | 1978.9 |
| BF16      | 133.8 | 989.4  | 1978.9 |
| FP8       |       | 1978.9 | 3957.8 |
| INT32     | 33.5  |        |        |
| INT8      |       | 1978.9 | 3957.8 |

You can also project the [[HPL]] performance of any H100-based supercomputer by looking at the TFLOPS/node or TFLOPS/GPU from the Top500 list and linearly extrapolate.

[^1]: [NVIDIA H100 Tensor Core GPU Datasheet](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid)